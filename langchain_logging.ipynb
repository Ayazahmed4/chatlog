
# Install required libraries if not already installed
# !pip install langchain openai azure-identity pandas numpy

# Import necessary libraries
import os
import re
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Tuple, Optional
import asyncio

# Import Azure and LangChain libraries
from azure.identity import DefaultAzureCredential
import openai
from langchain.llms import AzureOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.agents import initialize_agent, AgentType
from langchain import PromptTemplate, LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.tools import BaseTool

# Set up OpenAI API to use Azure OpenAI Service
openai.api_type = "azure"
openai.api_version = "2023-05-15"  # Use the appropriate API version
openai.api_base = "https://<your-resource-name>.openai.azure.com/"  # Replace with your endpoint
openai.api_key = "<your-api-key>"  # Replace with your API key



class LogFilterTool(BaseTool):
    name = "Time-Based Log Filter"
    description = "Filters log entries based on a specified time frame."

    def __init__(self, log_file_path: str):
        self.log_file_path = log_file_path

    def _run(self, start_time: Optional[str] = None, end_time: Optional[str] = None) -> List[str]:
        start_datetime, end_datetime = self.get_time_frame(start_time, end_time)
        filtered_logs = self.filter_logs_by_time(start_datetime, end_datetime)
        return filtered_logs

    def _arun(self, *args, **kwargs):
        raise NotImplementedError("Async not supported for LogFilterTool.")

    def get_time_frame(self, start_time: Optional[str], end_time: Optional[str]) -> Tuple[datetime, datetime]:
        if end_time:
            end_datetime = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')
        else:
            end_datetime = datetime.now()
        if start_time:
            start_datetime = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')
        else:
            start_datetime = end_datetime - timedelta(days=1)
        return start_datetime, end_datetime

    def filter_logs_by_time(self, start_datetime: datetime, end_datetime: datetime) -> List[str]:
        filtered_logs = []
        timestamp_pattern = re.compile(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}')

        with open(self.log_file_path, 'r') as file:
            for line in file:
                match = timestamp_pattern.match(line)
                if match:
                    log_time_str = match.group()
                    log_time = datetime.strptime(log_time_str, '%Y-%m-%d %H:%M:%S')
                    if start_datetime <= log_time <= end_datetime:
                        filtered_logs.append(line)
                else:
                    if filtered_logs:
                        filtered_logs[-1] += line
        return filtered_logs


# Define the path to your log file
log_file_path = 'system_log.txt'  # Replace with your actual log file path

# Initialize the tool
log_filter_tool = LogFilterTool(log_file_path=log_file_path)



class LogChunkerTool(BaseTool):
    name = "Log Chunker"
    description = "Chunks log entries into manageable sizes for processing."

    def _run(self, log_entries: List[str], chunk_size: int = 1000) -> List[List[str]]:
        chunks = self.chunk_logs(log_entries, chunk_size)
        return chunks

    def _arun(self, *args, **kwargs):
        raise NotImplementedError("Async not supported for LogChunkerTool.")

    def chunk_logs(self, log_entries: List[str], chunk_size: int) -> List[List[str]]:
        chunks = [log_entries[i:i + chunk_size] for i in range(0, len(log_entries), chunk_size)]
        return chunks




log_chunker_tool = LogChunkerTool()



# Configure the Azure OpenAI LLM
llm = AzureOpenAI(
    deployment_name="<your-deployment-name>",  # Replace with your deployment name
    model_name="gpt-35-turbo",  # Replace with your model name if different
    openai_api_key=openai.api_key,
    openai_api_base=openai.api_base,
    openai_api_version=openai.api_version,
    openai_api_type=openai.api_type
)



class LogSummarizerTool(BaseTool):
    name = "Log Summarizer"
    description = "Summarizes chunks of log entries."

    def __init__(self, llm):
        self.llm = llm

    def _run(self, chunks: List[List[str]]) -> List[str]:
        summaries = []
        for idx, chunk in enumerate(chunks):
            print(f"Summarizing chunk {idx + 1}/{len(chunks)}...")
            summary = self.summarize_chunk(chunk)
            summaries.append(summary)
        return summaries

    def _arun(self, *args, **kwargs):
        raise NotImplementedError("Async not supported for LogSummarizerTool.")

    def summarize_chunk(self, chunk: List[str]) -> str:
        chunk_text = ''.join(chunk)
        prompt = f"Summarize the following log entries, highlighting any errors, warnings, and unusual activities:\n\n{chunk_text}\n\nSummary:"
        try:
            response = self.llm(prompt)
            return response.strip()
        except Exception as e:
            print(f"Error during summarization: {e}")
            return ""



log_summarizer_tool = LogSummarizerTool(llm=llm)



# List of tools the agent can use
tools = [
    log_filter_tool,
    log_chunker_tool,
    log_summarizer_tool,
    # You can add more tools as needed
]



# Initialize memory for the agent
memory = ConversationBufferMemory(memory_key="chat_history")

# Initialize the agent
agent_chain = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    memory=memory
)




def process_user_request(agent_chain, start_time: Optional[str] = None, end_time: Optional[str] = None, chunk_size: int = 1000):
    # Step 1: Filter logs
    filtered_logs = log_filter_tool._run(start_time=start_time, end_time=end_time)
    print(f"Number of log entries after filtering: {len(filtered_logs)}")

    # Step 2: Chunk logs
    log_chunks = log_chunker_tool._run(log_entries=filtered_logs, chunk_size=chunk_size)
    print(f"Total number of chunks: {len(log_chunks)}")

    # Step 3: Summarize chunks
    summaries = log_summarizer_tool._run(chunks=log_chunks)
    print(f"Generated {len(summaries)} summaries.")

    # Step 4: Present summaries or further process as needed
    return summaries





# User selects date (or defaults to last 24 hours)
user_start_time = None  # e.g., '2023-10-08 00:00:00'
user_end_time = None    # e.g., '2023-10-09 00:00:00'
user_chunk_size = 500   # Adjust as needed

# Process the request
summaries = process_user_request(
    agent_chain=agent_chain,
    start_time=user_start_time,
    end_time=user_end_time,
    chunk_size=user_chunk_size
)

# Display summaries
for idx, summary in enumerate(summaries):
    print(f"Summary for Chunk {idx + 1}:\n{summary}\n{'-'*80}\n")





